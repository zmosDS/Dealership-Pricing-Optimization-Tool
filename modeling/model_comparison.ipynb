{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f57bc1d-78d2-4e24-a591-439a6e6848ce",
   "metadata": {},
   "source": [
    "# Model Comparison\n",
    "\n",
    "## Objective\n",
    "The objective of this notebook is to compare the performance of various machine learning models for predicting car prices, ultimately identifying the top-performing models for final optimization and deployment.\n",
    "\n",
    "## Summary\n",
    "The comparison of multiple machine learning models, including Random Forest, XGBoost, CatBoost, and LightGBM, revealed that **LightGBM** outperformed all others in terms of predictive accuracy and computational efficiency. \n",
    "\n",
    "### Key Findings\n",
    "- **LightGBM:**\n",
    "  - Mean Absolute Error (MAE): `$1,695.51`\n",
    "  - Mean Squared Error (MSE): `8,147,484`\n",
    "  - R² Score: `0.9532`\n",
    "  - **Performance Summary:** LightGBM showed the best overall performance, with the lowest MAE and MSE, and the highest R² score. It also had the fastest execution time, making it the top choice for further optimization and final deployment.\n",
    "\n",
    "### Final Model Recommendation\n",
    "Moving forward, **LightGBM** will be the model of choice for further tuning and validation. Its combination of high accuracy, speed, and effective handling of categorical data makes it the optimal candidate for deployment in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0145597c-3fe3-44c4-a81f-286a4a8c93f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Necessary libraries\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be545e62-d2ae-440b-bcd7-ca5b6f818223",
   "metadata": {},
   "source": [
    "### Import and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ae23d-10c3-40a1-a700-95a7c0e1d759",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported & processed\n",
      "\n",
      "Train & Test Data Row, Column Count: (66316, 4221) (16579, 4221)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df_cars = pd.read_csv('cleaned_data_july_21st.csv')\n",
    "\n",
    "# Define features and save column names\n",
    "features = ['Year', 'Model', 'State', 'Mileage', 'Trim', 'Make', 'Body Style', 'City']  # City increase features by more than 3,000\n",
    "column_names = pd.get_dummies(df_cars[features], drop_first=False) # Fixes numpy array error\n",
    "\n",
    "# Encode features and define target\n",
    "X = pd.get_dummies(df_cars[features], drop_first=False).values\n",
    "y = df_cars['Price'].values.reshape(-1)\n",
    "\n",
    "# Define scaler and scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=column_names.columns) # Convert back to DataFrame to maintain column names\n",
    "\n",
    "# Split data into 80/20 train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=.2, random_state=1)\n",
    "\n",
    "print('Data imported & processed')\n",
    "print('\\nTrain & Test Data Row, Column Count:', X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d2e082-4b19-40d1-8878-23eff7535fa2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Model Comparison\n",
    "(Preliminary Model Performance)\n",
    "\n",
    "### Linear Regression (Baseline Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104af758-be8e-4425-9466-ac0decf1d6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLinear Regression Performance:\u001b[0m\n",
      "Mean Absolute Error (MAE):  26721.3905543157\n",
      "Mean Squared Error  (MSE):  888169699.3344593\n",
      "R2 Score             (R2):  -4.100408125966033\n",
      "871.5 seconds to execute.\n",
      "\n",
      "\n",
      "intercept  nan\n",
      "            Predictor   coefficient\n",
      "0                Year  4.366843e+03\n",
      "1             Mileage -3.678320e+03\n",
      "2          Model_370z -5.752513e+13\n",
      "3       Model_4runner  6.883538e+14\n",
      "4            Model_86  4.909451e+13\n",
      "...               ...           ...\n",
      "4216    City_oak lawn           NaN\n",
      "4217   City_rochester           NaN\n",
      "4218  City_scottsdale           NaN\n",
      "4219    City_stockton           NaN\n",
      "4220      City_warren           NaN\n",
      "\n",
      "[4221 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Track training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train Linear regression model\n",
    "Linear_model = LinearRegression(n_jobs=-1)\n",
    "Linear_model.fit(X_train,y_train.ravel())\n",
    "pred_linear = Linear_model.predict(X_test)\n",
    "\n",
    "# Replace NaN values in predictions with zero, fixes NaN error\n",
    "pred_linear = np.nan_to_num(pred_linear, nan=0.0)\n",
    "\n",
    "# Print performance\n",
    "print(\"\\033[1mLinear Regression Performance:\\033[0m\")\n",
    "print(\"Mean Absolute Error (MAE): \", metrics.mean_absolute_error(y_test, pred_linear))\n",
    "print(\"Mean Squared Error  (MSE): \", metrics.mean_squared_error(y_test, pred_linear))\n",
    "print(\"R2 Score             (R2): \", metrics.r2_score(y_test, pred_linear))\n",
    "\n",
    "# Print model execution time\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time)\n",
    "print(f\"{elapsed_time:.1f} seconds to execute.\")\n",
    "print('\\n')\n",
    "\n",
    "# Print coefficients\n",
    "print('intercept ', Linear_model.intercept_)\n",
    "print(pd.DataFrame({'Predictor': X_scaled.columns, 'coefficient': Linear_model.coef_}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137377e1-c3fa-47a4-8e26-00c317aa80f2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc330c94-f87e-4904-84f0-61e9bcc539e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mRandom Forest Regression Performance:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 1,832.50\n",
      "Mean Squared Error  (MSE): 9,145,979\n",
      "R2 Score             (R2): 0.9475\n",
      "338.0 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "# Track training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train Random Forest regressor model\n",
    "RF_model = RandomForestRegressor(n_jobs=-1)\n",
    "RF_model.fit(X_train,y_train.ravel())\n",
    "pred_RF = RF_model.predict(X_test)\n",
    "\n",
    "# Print performance\n",
    "print(\"\\033[1mRandom Forest Regression Performance:\\033[0m\")\n",
    "print(f'Mean Absolute Error (MAE): $ {metrics.mean_absolute_error(y_test, pred_RF):,.2f}')\n",
    "print(f'Mean Squared Error  (MSE): {int(metrics.mean_squared_error(y_test, pred_RF)):,}')\n",
    "print(f'R2 Score             (R2): {metrics.r2_score(y_test, pred_RF):.4f}')\n",
    "\n",
    "# Print model execution time\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time)\n",
    "print(f\"{elapsed_time:.1f} seconds to execute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512922e5-811b-4dd5-ab1a-dd4375cb6ec0",
   "metadata": {},
   "source": [
    "### XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6a215f-7b58-4f3e-9fc4-08a23efcd425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mXGBoost Regression Performance:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 2,247.27\n",
      "Mean Squared Error  (MSE): 11,244,244\n",
      "R2 Score             (R2): 0.9354\n",
      "9.9 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "# Track training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train XGBoost regressor model\n",
    "XGBoost_model = xgb.XGBRegressor()\n",
    "XGBoost_model.fit(X_train, y_train.ravel())\n",
    "pred_XGBoost = XGBoost_model.predict(X_test)\n",
    "\n",
    "# Print performance\n",
    "print(\"\\033[1mXGBoost Regression Performance:\\033[0m\")\n",
    "print(f'Mean Absolute Error (MAE): $ {metrics.mean_absolute_error(y_test, pred_XGBoost):,.2f}')\n",
    "print(f'Mean Squared Error  (MSE): {int(metrics.mean_squared_error(y_test, pred_XGBoost)):,}')\n",
    "print(f'R2 Score             (R2): {metrics.r2_score(y_test, pred_XGBoost):.4f}')\n",
    "\n",
    "# Print model execution time\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time)\n",
    "print(f\"{elapsed_time:.1f} seconds to execute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bbfa06-2a41-4d1a-a3ec-cd8667ecab4b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6799dc42-dfa0-4907-8116-75798f9367e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mRidge Regression Performance:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 2,574.89\n",
      "Mean Squared Error  (MSE): 15,995,740\n",
      "R2 Score             (R2): 0.9081\n",
      "437.4 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "# Track training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train Ridge regression model (cv for more accurate score)\n",
    "Ridge_model = RidgeCV(alphas=np.logspace(-6, 6, 13), cv=5) #logspace finds the best alpha with ridgecv\n",
    "Ridge_model.fit(X_train,y_train)\n",
    "pred_Ridge = Ridge_model.predict(X_test)\n",
    "\n",
    "# Print performance\n",
    "print(\"\\033[1mRidge Regression Performance:\\033[0m\")\n",
    "print(f'Mean Absolute Error (MAE): $ {metrics.mean_absolute_error(y_test, pred_Ridge):,.2f}')\n",
    "print(f'Mean Squared Error  (MSE): {int(metrics.mean_squared_error(y_test, pred_Ridge)):,}')\n",
    "print(f'R2 Score             (R2): {metrics.r2_score(y_test, pred_Ridge):.4f}')\n",
    "\n",
    "# Print model execution time\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time)\n",
    "print(f\"{elapsed_time:.1f} seconds to execute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd15e958-d95f-4115-9713-9d01fd05cfa6",
   "metadata": {},
   "source": [
    "### Lasso Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dafa2d-bdb2-4ff6-954c-c9623356bfa2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLasso Regression Performance:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 2,553.84\n",
      "Mean Squared Error  (MSE): 15,812,539\n",
      "R2 Score             (R2): 0.9092\n",
      "255.2 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "# Track training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train Lasso regression model\n",
    "Lasso_model = LassoCV(cv=5, random_state=1)\n",
    "Lasso_model.fit(X_train,y_train.ravel())\n",
    "pred_Lasso = Lasso_model.predict(X_test)\n",
    "\n",
    "# Print performance\n",
    "print(\"\\033[1mLasso Regression Performance:\\033[0m\")\n",
    "print(f'Mean Absolute Error (MAE): $ {metrics.mean_absolute_error(y_test, pred_Lasso):,.2f}')\n",
    "print(f'Mean Squared Error  (MSE): {int(metrics.mean_squared_error(y_test, pred_Lasso)):,}')\n",
    "print(f'R2 Score             (R2): {metrics.r2_score(y_test, pred_Lasso):.4f}')\n",
    "\n",
    "# Print model execution time\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time)\n",
    "print(f\"{elapsed_time:.1f} seconds to execute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efab533a-4d36-4caf-ac9a-46e2d41bd213",
   "metadata": {},
   "source": [
    "## **Conclusion** - Legacy Model Comparison\n",
    "\n",
    "The performance comparison of the four initial regression models (Random Forest, XGBoost, Ridge, and Lasso) for predicting car prices highlights Random Forest as the best overall performer. This model consistently demonstrated the lowest Mean Absolute Error (MAE) and Mean Squared Error (MSE) values, along with the highest R² score, indicating superior predictive accuracy and generalization capability.\n",
    "\n",
    "- **Random Forest:** With an MAE of `1,833` and an R² score of `0.9475`, the Random Forest model excelled in both error reduction and variance explanation, making it the strongest candidate for predicting vehicle values.\n",
    "  \n",
    "- **XGBoost:** While XGBoost’s MAE was higher at `$2,247` and its R² score slightly lower at `0.9354`, it still showed competitive performance. Given XGBoost’s reputation for improvement through hyperparameter tuning, I plan to explore further optimizations for this model to narrow the performance gap between it and Random Forest.\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Random Forest** is the most accurate model overall, delivering the best balance of predictive performance and efficiency.\n",
    "- **XGBoost** shows potential for further improvement through hyperparameter tuning, which might make it a more competitive choice in future iterations.\n",
    "- **Ridge and Lasso** models, while useful for regularization, demonstrated inferior performance relative to the tree-based models in this application, indicating that they may not be the best fit for this problem space.\n",
    "\n",
    "### Next Steps:\n",
    "The immediate next steps will involve fine-tuning the Random Forest and XGBoost models. I plan to focus on hyperparameter optimization for both models to further improve their performance. Given XGBoost’s potential for substantial improvements through tuning, the goal will be to achieve optimal results by comparing the post-optimization performance of these two models. Once this optimization is complete, the results will guide the final selection of the best model for deployment. \n",
    "\n",
    "### **The optimization and evaluation process for both the Random Forest and XGBoost models can be found in the `legacy_model.ipynb` notebook, where hyperparameter tuning and further analysis are documented.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbe0bf1-02b2-4ef7-a87d-2573d66d7174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3242f27f-c202-4c4a-bbb5-c71468e3cba2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# **Expanding to Compare Categorical Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda649dc-8058-46db-9b17-eab4915ab1ea",
   "metadata": {},
   "source": [
    "### Introduction to Categorical Models\n",
    "\n",
    "While working with traditional regression models like Random Forest and XGBoost, I initially handled categorical features using techniques such as one-hot encoding. However, as I sought to optimize my models further, I discovered that there are machine learning models specifically designed for datasets with categorical features, which could improve both performance and efficiency.\n",
    "\n",
    "These categorical models, such as CatBoost and LightGBM, natively handle categorical variables, allowing for more streamlined processing and faster computation without the need for manual encoding. This discovery presented an opportunity to explore these models as alternatives to the legacy models.\n",
    "\n",
    "### Transition to Categorical Model Comparison\n",
    "\n",
    "In the next section, I compare the performance of these categorical-specific models (XGBoost, CatBoost, LightGBM) with the previously optimized models. By leveraging these models, I aim to achieve better predictive accuracy and efficiency for my dataset, which consists predominantly of categorical features. I apply these models to the validation data before moving forward with optimization to ensure I only invest time in a model that is likely to improve on the legacy model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd67e2c-f12b-47b5-81dc-4cedd1b1ab8d",
   "metadata": {},
   "source": [
    "## Categorical Models without Hot-Encoding\n",
    "\n",
    "#### Process Data for Categorical Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fa7d44-44c6-46c1-840b-8a0cbca89032",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test Data Row & Column Count: (66316, 8) (16579, 8)\n",
      "\n",
      "Data processed for categorical model\n"
     ]
    }
   ],
   "source": [
    "# Define features and target\n",
    "features = ['Year', 'Model', 'State', 'Mileage', 'Trim', 'Make', 'Body Style', 'City']\n",
    "X_categorical = df_cars[features].copy()\n",
    "y_categorical = df_cars['Price']\n",
    "\n",
    "# Define categorical features\n",
    "categorical_features = ['Model', 'State', 'Trim', 'Make', 'Body Style', 'City']\n",
    "\n",
    "# Format categorical features\n",
    "X_categorical[categorical_features] = X_categorical[categorical_features].astype('category')\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_categorical[['Year', 'Mileage']] = scaler.fit_transform(X_categorical[['Year', 'Mileage']])\n",
    "joblib.dump(scaler, 'numerical_scaler.pkl') # Save numerical scaler\n",
    "\n",
    "# Split the data\n",
    "train_X, test_X, train_y, test_y = train_test_split(X_categorical, y_categorical, test_size=0.2, random_state=1)\n",
    "\n",
    "print('Train/Test Data Row & Column Count:', train_X.shape, test_X.shape)\n",
    "print('\\nData processed for categorical model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10f6503-3c9a-4277-87c2-85e2bcc7804f",
   "metadata": {},
   "source": [
    "## XGBoost Regressor (enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c5881-b244-43b6-9d8c-f95b5755c3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCategorical XGBoost Regressor Performance:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 1,820.59\n",
      "Mean Squared Error  (MSE): 9,253,507\n",
      "R2 Score             (R2): 0.9469\n",
      "1.4 seconds to execute.\n",
      "\u001b[1m\n",
      "Encoded XGBoost Regressor Performance:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 2,247.27\n",
      "Mean Squared Error  (MSE): 11,244,244\n",
      "R2 Score             (R2): 0.9354\n"
     ]
    }
   ],
   "source": [
    "# Track training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train XGBoost regressor (enable_categorical) model\n",
    "XGBoost_categorical_model = xgb.XGBRegressor(enable_categorical=True, n_jobs=-1)\n",
    "XGBoost_categorical_model.fit(train_X, train_y.to_numpy())\n",
    "pred_XGBoost_categorical = XGBoost_categorical_model.predict(test_X)\n",
    "\n",
    "# Print performance\n",
    "print(\"\\033[1mCategorical XGBoost Regressor Performance:\\033[0m\")\n",
    "print(f'Mean Absolute Error (MAE): $ {metrics.mean_absolute_error(test_y, pred_XGBoost_categorical):,.2f}')\n",
    "print(f'Mean Squared Error  (MSE): {int(metrics.mean_squared_error(test_y, pred_XGBoost_categorical)):,}')\n",
    "print(f'R2 Score             (R2): {metrics.r2_score(test_y, pred_XGBoost_categorical):.4f}')\n",
    "\n",
    "# Print model execution time\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time)\n",
    "print(f\"{elapsed_time:.1f} seconds to execute.\")\n",
    "\n",
    "# Print performance\n",
    "print(\"\\033[1m\\nEncoded XGBoost Regressor Performance:\\033[0m\")\n",
    "print(f'Mean Absolute Error (MAE): $ {metrics.mean_absolute_error(y_test, pred_XGBoost):,.2f}')\n",
    "print(f'Mean Squared Error  (MSE): {int(metrics.mean_squared_error(y_test, pred_XGBoost)):,}')\n",
    "print(f'R2 Score             (R2): {metrics.r2_score(y_test, pred_XGBoost):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b606a9-6831-426e-99b7-b4bca8dc25c7",
   "metadata": {},
   "source": [
    "### Apply Categorical XGBoost to Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a25cc75-9e3c-4f15-b74f-2363be320390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Data loaded and processed\n",
      "\n",
      "Validation Data Row and Column Count: (9229, 8)\n",
      "\n",
      "\u001b[1mCategorical XGBoost Regressor Performance on Validation Data from 8/15:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 8,773.75\n",
      "Mean Squared Error  (MSE): 164,607,404\n",
      "R2 Score             (R2): 0.0359\n",
      "\u001b[1m\n",
      "Categorical XGBoost Regressor Performance on Original Data from 7/21:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 1,820.59\n",
      "Mean Squared Error  (MSE): 9,253,507\n",
      "R2 Score             (R2): 0.9469\n"
     ]
    }
   ],
   "source": [
    "# Load validation data\n",
    "df_validation = pd.read_csv('cleaned_data_aug_16th.csv') \n",
    "\n",
    "# Format cateforical features\n",
    "df_validation[categorical_features] = df_validation[categorical_features].astype('category')\n",
    "\n",
    "# Scale and format numerical features\n",
    "X_validation = df_validation[features].copy()\n",
    "X_validation[['Year', 'Mileage']] = scaler.transform(X_validation[['Year', 'Mileage']])\n",
    "X_validation[['Year', 'Mileage']] = X_validation[['Year', 'Mileage']].astype('float64')\n",
    "\n",
    "print('Validation Data loaded and processed')\n",
    "print('\\nValidation Data Row and Column Count:', X_validation.shape)\n",
    "\n",
    "# Predict validation data\n",
    "pred_xgb = XGBoost_categorical_model.predict(X_validation)\n",
    "\n",
    "# Define validation target\n",
    "y_validation = df_validation['Price'].values\n",
    "\n",
    "# Print validation performance\n",
    "print('\\n\\033[1mCategorical XGBoost Regressor Performance on Validation Data from 8/15:\\033[0m')\n",
    "print(f'Mean Absolute Error (MAE): $ {round(metrics.mean_absolute_error(y_validation, pred_xgb), 2):,}')\n",
    "print(f'Mean Squared Error  (MSE): {int(round(metrics.mean_squared_error(y_validation, pred_xgb))):,}')\n",
    "print(f'R2 Score             (R2): {round(metrics.r2_score(y_validation, pred_xgb), 4)}')\n",
    "\n",
    "# Print original performance\n",
    "print(\"\\033[1m\\nCategorical XGBoost Regressor Performance on Original Data from 7/21:\\033[0m\")\n",
    "print(f'Mean Absolute Error (MAE): $ {metrics.mean_absolute_error(test_y, pred_XGBoost_categorical):,.2f}')\n",
    "print(f'Mean Squared Error  (MSE): {int(metrics.mean_squared_error(test_y, pred_XGBoost_categorical)):,}')\n",
    "print(f'R2 Score             (R2): {metrics.r2_score(test_y, pred_XGBoost_categorical):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42340193-1784-41fd-9278-6eb0ec0766c0",
   "metadata": {},
   "source": [
    "## Checkpoint: XGBoost Categorical Model Results\n",
    "\n",
    "After running into issues with my Encoded XGBoost modelshboard, I began to learn more about XGBoost and found out about the `enable_categorical=True` parameter that allows you to use categorical columns without encoding. I decided to try this parameter, which reduced the number of features from over 4,300 to only 8.The model performed well on the training data but was unusable on the validation set.\n",
    "\n",
    "This poor performance prompted further research into other categorical-specific models like CatBoost and LightGBM, which will be evaluated next.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fd5ad6-6d59-4c1c-b9dc-57fc45606dcc",
   "metadata": {},
   "source": [
    "## Catboost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3fecf-f4bc-4474-8015-b4a3161b5528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCatBoost Regressor Performance:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 1,922.83\n",
      "Mean Squared Error  (MSE): 10,072,576\n",
      "R2 Score             (R2): 0.9422\n",
      "14.7 seconds to execute.\n",
      "\n",
      "\u001b[1mFeature Importance:\u001b[0m\n",
      "      Feature  Importance\n",
      "1       Model   28.995478\n",
      "0        Year   18.955490\n",
      "4        Trim   15.433566\n",
      "3     Mileage   14.942127\n",
      "5        Make   13.957470\n",
      "6  Body Style    6.524228\n",
      "2       State    0.753334\n",
      "7        City    0.438307\n"
     ]
    }
   ],
   "source": [
    "# Track training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train Catboost regressor model\n",
    "catboost_model = CatBoostRegressor(cat_features=categorical_features, verbose=0)\n",
    "catboost_model.fit(train_X,train_y)\n",
    "pred_catboost = catboost_model.predict(test_X)\n",
    "\n",
    "# Print performance\n",
    "print(\"\\033[1mCatBoost Regressor Performance:\\033[0m\")\n",
    "print(f'Mean Absolute Error (MAE): $ {metrics.mean_absolute_error(test_y, pred_catboost):,.2f}')\n",
    "print(f'Mean Squared Error  (MSE): {int(metrics.mean_squared_error(test_y, pred_catboost)):,}')\n",
    "print(f'R2 Score             (R2): {metrics.r2_score(test_y, pred_catboost):.4f}')\n",
    "\n",
    "# Print model execution time\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time)\n",
    "print(f\"{elapsed_time:.1f} seconds to execute.\")\n",
    "\n",
    "# Print feature importance\n",
    "print(\"\\n\\033[1mFeature Importance:\\033[0m\")\n",
    "importance_catboost = pd.DataFrame({\n",
    "    'Feature': train_X.columns,\n",
    "    'Importance': catboost_model.get_feature_importance()\n",
    "})\n",
    "importance_catboost = importance_catboost.sort_values(by='Importance', ascending=False)\n",
    "print(importance_catboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae53785-60de-431d-9d2b-bbe4eef94712",
   "metadata": {},
   "source": [
    "## LightGBM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dec1013-8d36-4704-a2b8-87fefda5da20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLightGBM Regressor Performance:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 1,695.51\n",
      "Mean Squared Error  (MSE): 8,147,484\n",
      "R2 Score             (R2): 0.9532\n",
      "0.4 seconds to execute.\n",
      "\n",
      "\u001b[1mFeature Importance:\u001b[0m\n",
      "      Feature  Importance\n",
      "1       Model         685\n",
      "4        Trim         607\n",
      "3     Mileage         569\n",
      "7        City         533\n",
      "0        Year         384\n",
      "2       State         135\n",
      "6  Body Style          44\n",
      "5        Make          43\n"
     ]
    }
   ],
   "source": [
    "# Track training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train LightGBM regressor model\n",
    "light_model = lgb.LGBMRegressor(n_jobs=-1, verbose=-1)\n",
    "light_model.fit(train_X, train_y, categorical_feature=categorical_features)\n",
    "pred_light = light_model.predict(test_X)\n",
    "\n",
    "# Print performance\n",
    "print(\"\\033[1mLightGBM Regressor Performance:\\033[0m\")\n",
    "print(f'Mean Absolute Error (MAE): $ {metrics.mean_absolute_error(test_y, pred_light):,.2f}')\n",
    "print(f'Mean Squared Error  (MSE): {int(metrics.mean_squared_error(test_y, pred_light)):,}')\n",
    "print(f'R2 Score             (R2): {metrics.r2_score(test_y, pred_light):.4f}')\n",
    "\n",
    "# Print model execution time\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time)\n",
    "print(f\"{elapsed_time:.1f} seconds to execute.\")\n",
    "\n",
    "# Print feature importance\n",
    "print(\"\\n\\033[1mFeature Importance:\\033[0m\")\n",
    "importance_light = pd.DataFrame({\n",
    "    'Feature': train_X.columns,\n",
    "    'Importance': light_model.feature_importances_\n",
    "})\n",
    "importance_light = importance_light.sort_values(by='Importance', ascending=False)\n",
    "print(importance_light)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5d3432-b055-4499-8fcf-33e8aacc5ea5",
   "metadata": {},
   "source": [
    "### Apply CatBoost to Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14746fc0-d187-4ff8-8c78-44ec5a0f7a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mCatBoost Regressor Performance on Validation Data from 8/15:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 2,371.04\n",
      "Mean Squared Error  (MSE): 14,785,259\n",
      "R2 Score             (R2): 0.9134\n",
      "\u001b[1m\n",
      "CatBoost Regressor Performance on Original Data from 7/21:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 1,922.83\n",
      "Mean Squared Error  (MSE): 10,072,576\n",
      "R2 Score             (R2): 0.9422\n"
     ]
    }
   ],
   "source": [
    "# Predict validation data\n",
    "pred_catboost_val = catboost_model.predict(X_validation)\n",
    "\n",
    "# Print validation performance\n",
    "print('\\n\\033[1mCatBoost Regressor Performance on Validation Data from 8/15:\\033[0m')\n",
    "print(f'Mean Absolute Error (MAE): $ {round(metrics.mean_absolute_error(y_validation, pred_catboost_val), 2):,}')\n",
    "print(f'Mean Squared Error  (MSE): {int(round(metrics.mean_squared_error(y_validation, pred_catboost_val))):,}')\n",
    "print(f'R2 Score             (R2): {round(metrics.r2_score(y_validation, pred_catboost_val), 4)}')\n",
    "\n",
    "# Print original performance\n",
    "print(\"\\033[1m\\nCatBoost Regressor Performance on Original Data from 7/21:\\033[0m\")\n",
    "print(f'Mean Absolute Error (MAE): $ {metrics.mean_absolute_error(test_y, pred_catboost):,.2f}')\n",
    "print(f'Mean Squared Error  (MSE): {int(metrics.mean_squared_error(test_y, pred_catboost)):,}')\n",
    "print(f'R2 Score             (R2): {metrics.r2_score(test_y, pred_catboost):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc1035a-a2ad-4e46-9040-06d37d309db8",
   "metadata": {},
   "source": [
    "### Apply LightGBM to Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f62b31b-504a-429c-bb44-77d8958dea85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mLightGBM Regressor Performance on Validation Data from 8/15:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 1,898.75\n",
      "Mean Squared Error  (MSE): 10,302,178\n",
      "R2 Score             (R2): 0.9397\n",
      "\u001b[1m\n",
      "LightGBM Regressor Performance on Original Data from 7/21:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 1,695.51\n",
      "Mean Squared Error  (MSE): 8,147,484\n",
      "R2 Score             (R2): 0.9532\n"
     ]
    }
   ],
   "source": [
    "# Predict validation data using the loaded model\n",
    "pred_light_val = light_model.predict(X_validation)\n",
    "\n",
    "# Print validation performance\n",
    "print('\\n\\033[1mLightGBM Regressor Performance on Validation Data from 8/15:\\033[0m')\n",
    "print(f'Mean Absolute Error (MAE): $ {round(metrics.mean_absolute_error(y_validation, pred_light_val), 2):,}')\n",
    "print(f'Mean Squared Error  (MSE): {int(round(metrics.mean_squared_error(y_validation, pred_light_val))):,}')\n",
    "print(f'R2 Score             (R2): {round(metrics.r2_score(y_validation, pred_light_val), 4)}')\n",
    "\n",
    "# Print original performance\n",
    "print(\"\\033[1m\\nLightGBM Regressor Performance on Original Data from 7/21:\\033[0m\")\n",
    "print(f'Mean Absolute Error (MAE): $ {metrics.mean_absolute_error(test_y, pred_light):,.2f}')\n",
    "print(f'Mean Squared Error  (MSE): {int(metrics.mean_squared_error(test_y, pred_light)):,}')\n",
    "print(f'R2 Score             (R2): {metrics.r2_score(test_y, pred_light):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34eec3e-df4d-46c1-88e7-9a2d4af6df36",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "\n",
    "The goal of this notebook was to compare the performance of various machine learning models to predict car prices. Throughout this process, both traditional models and categorical-specific models were evaluated, with an emphasis on improving the handling of categorical data.\n",
    "\n",
    "Among all the models explored, **LightGBM** outperformed the rest, offering the best balance between accuracy, speed, and efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
