{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f57bc1d-78d2-4e24-a591-439a6e6848ce",
   "metadata": {},
   "source": [
    "# Model Comparison\n",
    "\n",
    "## Objective\n",
    "The objective of this notebook is to compare the performance of various machine learning models for predicting car prices, identifying the top-performing models for further optimization.\n",
    "\n",
    "## Summary\n",
    "The comparison of six machine learning models revealed that Random Forest and XGBoost were the top performers. Random Forest exhibited the best overall performance, while XGBoost showed potential for improvement through hyperparameter tuning.\n",
    "\n",
    "### Key Findings\n",
    "- **Random Forest:**\n",
    "  - Mean Absolute Error (MAE): `1,824.63`\n",
    "  - Mean Squared Error  (MSE): `8,395,573`\n",
    "  - R2 Score             (R2): `0.9515`\n",
    "  - **Performance Summary:** Random Forest outperformed the other models overall, demonstrating the lowest MAE and MSE, indicating higher accuracy and better generalization.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- **XGBoost:**\n",
    "  - Mean Absolute Error (MAE): `2,216.44`\n",
    "  - Mean Squared Error  (MSE): `10,367,387`\n",
    "  - R2 Score             (R2): `0.9402`\n",
    "  - **Performance Summary:** XGBoost showed competitive performance with a slightly lower R2 score compared to Random Forest. However, it is known for responding well to hyperparameter tuning, which could potentially improve its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0145597c-3fe3-44c4-a81f-286a4a8c93f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Necessary libraries\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be545e62-d2ae-440b-bcd7-ca5b6f818223",
   "metadata": {},
   "source": [
    "### Import and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ae23d-10c3-40a1-a700-95a7c0e1d759",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported & processed\n",
      "\n",
      "Train & Test Data Row, Column Count: (66316, 4221) (16579, 4221)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df_cars = pd.read_csv('cleaned_data_july_21st.csv')\n",
    "\n",
    "# Define features and save column names\n",
    "features = ['Year', 'Model', 'State', 'Mileage', 'Trim', 'Make', 'Body Style', 'City']  # City increase features by more than 3,000\n",
    "column_names = pd.get_dummies(df_cars[features], drop_first=False) # Fixes numpy array error\n",
    "\n",
    "# Encode features and define target\n",
    "X = pd.get_dummies(df_cars[features], drop_first=False).values\n",
    "y = df_cars['Price'].values.reshape(-1)\n",
    "\n",
    "# Define scaler and scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=column_names.columns) # Convert back to DataFrame to maintain column names\n",
    "\n",
    "# Split data into 80/20 train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=.2, random_state=1)\n",
    "\n",
    "print('Data imported & processed')\n",
    "print('\\nTrain & Test Data Row, Column Count:', X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d2e082-4b19-40d1-8878-23eff7535fa2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Model Comparison\n",
    "(Preliminary Model Performance)\n",
    "\n",
    "### Linear Regression (Baseline Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104af758-be8e-4425-9466-ac0decf1d6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLinear Regression Performance:\u001b[0m\n",
      "Mean Absolute Error (MAE):  26721.3905543157\n",
      "Mean Squared Error  (MSE):  888169699.3344593\n",
      "R2 Score             (R2):  -4.100408125966033\n",
      "871.5 seconds to execute.\n",
      "\n",
      "\n",
      "intercept  nan\n",
      "            Predictor   coefficient\n",
      "0                Year  4.366843e+03\n",
      "1             Mileage -3.678320e+03\n",
      "2          Model_370z -5.752513e+13\n",
      "3       Model_4runner  6.883538e+14\n",
      "4            Model_86  4.909451e+13\n",
      "...               ...           ...\n",
      "4216    City_oak lawn           NaN\n",
      "4217   City_rochester           NaN\n",
      "4218  City_scottsdale           NaN\n",
      "4219    City_stockton           NaN\n",
      "4220      City_warren           NaN\n",
      "\n",
      "[4221 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Track training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train Linear regression model\n",
    "Linear_model = LinearRegression(n_jobs=-1)\n",
    "Linear_model.fit(X_train,y_train.ravel())\n",
    "pred_linear = Linear_model.predict(X_test)\n",
    "\n",
    "# Replace NaN values in predictions with zero, fixes NaN error\n",
    "pred_linear = np.nan_to_num(pred_linear, nan=0.0)\n",
    "\n",
    "# Print performance\n",
    "print(\"\\033[1mLinear Regression Performance:\\033[0m\")\n",
    "print(\"Mean Absolute Error (MAE): \", metrics.mean_absolute_error(y_test, pred_linear))\n",
    "print(\"Mean Squared Error  (MSE): \", metrics.mean_squared_error(y_test, pred_linear))\n",
    "print(\"R2 Score             (R2): \", metrics.r2_score(y_test, pred_linear))\n",
    "\n",
    "# Print model execution time\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time)\n",
    "print(f\"{elapsed_time:.1f} seconds to execute.\")\n",
    "print('\\n')\n",
    "\n",
    "# Print coefficients\n",
    "print('intercept ', Linear_model.intercept_)\n",
    "print(pd.DataFrame({'Predictor': X_scaled.columns, 'coefficient': Linear_model.coef_}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137377e1-c3fa-47a4-8e26-00c317aa80f2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc330c94-f87e-4904-84f0-61e9bcc539e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mRandom Forest Regression Performance:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 1,832.50\n",
      "Mean Squared Error  (MSE): 9,145,979\n",
      "R2 Score             (R2): 0.9475\n",
      "338.0 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "# Track training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train Random Forest regressor model\n",
    "RF_model = RandomForestRegressor(n_jobs=-1)\n",
    "RF_model.fit(X_train,y_train.ravel())\n",
    "pred_RF = RF_model.predict(X_test)\n",
    "\n",
    "# Print performance\n",
    "print(\"\\033[1mRandom Forest Regression Performance:\\033[0m\")\n",
    "print(f'Mean Absolute Error (MAE): $ {metrics.mean_absolute_error(y_test, pred_RF):,.2f}')\n",
    "print(f'Mean Squared Error  (MSE): {int(metrics.mean_squared_error(y_test, pred_RF)):,}')\n",
    "print(f'R2 Score             (R2): {metrics.r2_score(y_test, pred_RF):.4f}')\n",
    "\n",
    "# Print model execution time\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time)\n",
    "print(f\"{elapsed_time:.1f} seconds to execute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512922e5-811b-4dd5-ab1a-dd4375cb6ec0",
   "metadata": {},
   "source": [
    "### XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6a215f-7b58-4f3e-9fc4-08a23efcd425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mXGBoost Regression Performance:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 2,247.27\n",
      "Mean Squared Error  (MSE): 11,244,244\n",
      "R2 Score             (R2): 0.9354\n",
      "9.9 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "# Track training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train XGBoost regressor model\n",
    "XGBoost_model = xgb.XGBRegressor()\n",
    "XGBoost_model.fit(X_train, y_train.ravel())\n",
    "pred_XGBoost = XGBoost_model.predict(X_test)\n",
    "\n",
    "# Print performance\n",
    "print(\"\\033[1mXGBoost Regression Performance:\\033[0m\")\n",
    "print(f'Mean Absolute Error (MAE): $ {metrics.mean_absolute_error(y_test, pred_XGBoost):,.2f}')\n",
    "print(f'Mean Squared Error  (MSE): {int(metrics.mean_squared_error(y_test, pred_XGBoost)):,}')\n",
    "print(f'R2 Score             (R2): {metrics.r2_score(y_test, pred_XGBoost):.4f}')\n",
    "\n",
    "# Print model execution time\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time)\n",
    "print(f\"{elapsed_time:.1f} seconds to execute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bbfa06-2a41-4d1a-a3ec-cd8667ecab4b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6799dc42-dfa0-4907-8116-75798f9367e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mRidge Regression Performance:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 2,574.89\n",
      "Mean Squared Error  (MSE): 15,995,740\n",
      "R2 Score             (R2): 0.9081\n",
      "437.4 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "# Track training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train Ridge regression model (cv for more accurate score)\n",
    "Ridge_model = RidgeCV(alphas=np.logspace(-6, 6, 13), cv=5) #logspace finds the best alpha with ridgecv\n",
    "Ridge_model.fit(X_train,y_train)\n",
    "pred_Ridge = Ridge_model.predict(X_test)\n",
    "\n",
    "# Print performance\n",
    "print(\"\\033[1mRidge Regression Performance:\\033[0m\")\n",
    "print(f'Mean Absolute Error (MAE): $ {metrics.mean_absolute_error(y_test, pred_Ridge):,.2f}')\n",
    "print(f'Mean Squared Error  (MSE): {int(metrics.mean_squared_error(y_test, pred_Ridge)):,}')\n",
    "print(f'R2 Score             (R2): {metrics.r2_score(y_test, pred_Ridge):.4f}')\n",
    "\n",
    "# Print model execution time\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time)\n",
    "print(f\"{elapsed_time:.1f} seconds to execute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd15e958-d95f-4115-9713-9d01fd05cfa6",
   "metadata": {},
   "source": [
    "### Lasso Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dafa2d-bdb2-4ff6-954c-c9623356bfa2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLasso Regression Performance:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 2,553.84\n",
      "Mean Squared Error  (MSE): 15,812,539\n",
      "R2 Score             (R2): 0.9092\n",
      "255.2 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "# Track training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train Lasso regression model\n",
    "Lasso_model = LassoCV(cv=5, random_state=1)\n",
    "Lasso_model.fit(X_train,y_train.ravel())\n",
    "pred_Lasso = Lasso_model.predict(X_test)\n",
    "\n",
    "# Print performance\n",
    "print(\"\\033[1mLasso Regression Performance:\\033[0m\")\n",
    "print(f'Mean Absolute Error (MAE): $ {metrics.mean_absolute_error(y_test, pred_Lasso):,.2f}')\n",
    "print(f'Mean Squared Error  (MSE): {int(metrics.mean_squared_error(y_test, pred_Lasso)):,}')\n",
    "print(f'R2 Score             (R2): {metrics.r2_score(y_test, pred_Lasso):.4f}')\n",
    "\n",
    "# Print model execution time\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time)\n",
    "print(f\"{elapsed_time:.1f} seconds to execute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efab533a-4d36-4caf-ac9a-46e2d41bd213",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Conclusion - Ininital Preliminary Model Comparison\n",
    "\n",
    "The Random Forest model had the best overall performance, with the lowest error metrics and the highest R2 score, making it a strong candidate for predicting car prices. However, the XGBoost model, despite slightly higher error metrics, demonstrated a competitive R2 score, suggesting it captures a substantial amount of variance. Given XGBoost’s potential for significant improvements through hyperparameter tuning, I have decided to move forward with optimizing both the Random Forest and XGBoost models. The goal is to achieve the highest possible predictive performance by fine-tuning these models and comparing their performance post-optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3242f27f-c202-4c4a-bbb5-c71468e3cba2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# New findings \n",
    "\n",
    "Below are models that i learned about while trying to optimize other models. initially I was moving forward with XGboost, then I found out that XGboost has a feature to enable cateforical columns. 6 of my 8 features are categorical so this was a great feature. My optimized XGboost model performed slightly worse than the hot-encoded xgboost model but when applied to the validation data it was significantly better. I then had trouble with creating my dashboard with that model. While researching my problem, i found out about catboost, a model made specifically for categorical features, and lightxgm, another boosted gradient model. All of these models are able to handle the data without encoding, significantly speeding up the efficiency. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd67e2c-f12b-47b5-81dc-4cedd1b1ab8d",
   "metadata": {},
   "source": [
    "## Categorical Models without Hot-Encoding\n",
    "\n",
    "#### Process Data for Categorical Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fa7d44-44c6-46c1-840b-8a0cbca89032",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test Data Row & Column Count: (66316, 8) (16579, 8)\n",
      "\n",
      "Data processed for categorical model\n"
     ]
    }
   ],
   "source": [
    "# Define features and target\n",
    "features = ['Year', 'Model', 'State', 'Mileage', 'Trim', 'Make', 'Body Style', 'City']\n",
    "X_categorical = df_cars[features].copy()\n",
    "y_categorical = df_cars['Price']\n",
    "\n",
    "# Define categorical features\n",
    "categorical_features = ['Model', 'State', 'Trim', 'Make', 'Body Style', 'City']\n",
    "\n",
    "# Format categorical features\n",
    "X_categorical[categorical_features] = X_categorical[categorical_features].astype('category')\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_categorical[['Year', 'Mileage']] = scaler.fit_transform(X_categorical[['Year', 'Mileage']])\n",
    "joblib.dump(scaler, 'numerical_scaler.pkl') # Save numerical scaler\n",
    "\n",
    "# Split the data\n",
    "train_X, test_X, train_y, test_y = train_test_split(X_categorical, y_categorical, test_size=0.2, random_state=1)\n",
    "\n",
    "print('Train/Test Data Row & Column Count:', train_X.shape, test_X.shape)\n",
    "print('\\nData processed for categorical model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10f6503-3c9a-4277-87c2-85e2bcc7804f",
   "metadata": {},
   "source": [
    "## XGBoost Regressor (enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c5881-b244-43b6-9d8c-f95b5755c3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCategorical XGBoost Regressor Performance:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 1,820.59\n",
      "Mean Squared Error  (MSE): 9,253,507\n",
      "R2 Score             (R2): 0.9469\n",
      "1.4 seconds to execute.\n",
      "\u001b[1m\n",
      "Encoded XGBoost Regressor Performance:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 2,247.27\n",
      "Mean Squared Error  (MSE): 11,244,244\n",
      "R2 Score             (R2): 0.9354\n"
     ]
    }
   ],
   "source": [
    "# Track training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train XGBoost regressor (enable_categorical) model\n",
    "XGBoost_categorical_model = xgb.XGBRegressor(enable_categorical=True, n_jobs=-1)\n",
    "XGBoost_categorical_model.fit(train_X, train_y.to_numpy())\n",
    "pred_XGBoost_categorical = XGBoost_categorical_model.predict(test_X)\n",
    "\n",
    "# Print performance\n",
    "print(\"\\033[1mCategorical XGBoost Regressor Performance:\\033[0m\")\n",
    "print(f'Mean Absolute Error (MAE): $ {metrics.mean_absolute_error(test_y, pred_XGBoost_categorical):,.2f}')\n",
    "print(f'Mean Squared Error  (MSE): {int(metrics.mean_squared_error(test_y, pred_XGBoost_categorical)):,}')\n",
    "print(f'R2 Score             (R2): {metrics.r2_score(test_y, pred_XGBoost_categorical):.4f}')\n",
    "\n",
    "# Print model execution time\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time)\n",
    "print(f\"{elapsed_time:.1f} seconds to execute.\")\n",
    "\n",
    "# Print performance\n",
    "print(\"\\033[1m\\nEncoded XGBoost Regressor Performance:\\033[0m\")\n",
    "print(f'Mean Absolute Error (MAE): $ {metrics.mean_absolute_error(y_test, pred_XGBoost):,.2f}')\n",
    "print(f'Mean Squared Error  (MSE): {int(metrics.mean_squared_error(y_test, pred_XGBoost)):,}')\n",
    "print(f'R2 Score             (R2): {metrics.r2_score(y_test, pred_XGBoost):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b606a9-6831-426e-99b7-b4bca8dc25c7",
   "metadata": {},
   "source": [
    "### Apply Categorical XGBoost to Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a25cc75-9e3c-4f15-b74f-2363be320390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Data loaded and processed\n",
      "\n",
      "Validation Data Row and Column Count: (9229, 8)\n",
      "\n",
      "\u001b[1mCategorical XGBoost Regressor Performance on Validation Data from 8/15:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 8,773.75\n",
      "Mean Squared Error  (MSE): 164,607,404\n",
      "R2 Score             (R2): 0.0359\n",
      "\u001b[1m\n",
      "Categorical XGBoost Regressor Performance on Original Data from 7/21:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 1,820.59\n",
      "Mean Squared Error  (MSE): 9,253,507\n",
      "R2 Score             (R2): 0.9469\n"
     ]
    }
   ],
   "source": [
    "# Load validation data\n",
    "df_validation = pd.read_csv('cleaned_data_aug_16th.csv') \n",
    "\n",
    "# Format cateforical features\n",
    "df_validation[categorical_features] = df_validation[categorical_features].astype('category')\n",
    "\n",
    "# Scale and format numerical features\n",
    "X_validation = df_validation[features].copy()\n",
    "X_validation[['Year', 'Mileage']] = scaler.transform(X_validation[['Year', 'Mileage']])\n",
    "X_validation[['Year', 'Mileage']] = X_validation[['Year', 'Mileage']].astype('float64')\n",
    "\n",
    "print('Validation Data loaded and processed')\n",
    "print('\\nValidation Data Row and Column Count:', X_validation.shape)\n",
    "\n",
    "# Predict validation data\n",
    "pred_xgb = XGBoost_categorical_model.predict(X_validation)\n",
    "\n",
    "# Define validation target\n",
    "y_validation = df_validation['Price'].values\n",
    "\n",
    "# Print validation performance\n",
    "print('\\n\\033[1mCategorical XGBoost Regressor Performance on Validation Data from 8/15:\\033[0m')\n",
    "print(f'Mean Absolute Error (MAE): $ {round(metrics.mean_absolute_error(y_validation, pred_xgb), 2):,}')\n",
    "print(f'Mean Squared Error  (MSE): {int(round(metrics.mean_squared_error(y_validation, pred_xgb))):,}')\n",
    "print(f'R2 Score             (R2): {round(metrics.r2_score(y_validation, pred_xgb), 4)}')\n",
    "\n",
    "# Print original performance\n",
    "print(\"\\033[1m\\nCategorical XGBoost Regressor Performance on Original Data from 7/21:\\033[0m\")\n",
    "print(f'Mean Absolute Error (MAE): $ {metrics.mean_absolute_error(test_y, pred_XGBoost_categorical):,.2f}')\n",
    "print(f'Mean Squared Error  (MSE): {int(metrics.mean_squared_error(test_y, pred_XGBoost_categorical)):,}')\n",
    "print(f'R2 Score             (R2): {metrics.r2_score(test_y, pred_XGBoost_categorical):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42340193-1784-41fd-9278-6eb0ec0766c0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "After running into issues with my Encoded XGBoost model in the dashboard, I began to learn more about XGBoost and found out about the parameters that allows you to use categorical columns without encoding. I decided to try this parameters, enable_categorical=True, this took the number of features from over 4,300 to only 8. This model performed significantly faster and more accurate without any hyperparameter tuning. I moved forward with hyperparameter tuning and validation. The model performed extremely well everywhere until i applied it to the validation set. This model was unusable on the validation data that introduced a small number of new models and trims, as you can see above when apply untuned model to validation data. I was going to go back to my encoded xgboost model but decided to do more research if there are any other models that accept categorical columns without encoding. I came accross CatBoost and LightGBM. Below i compare the 2 models and apply them to the validation data before deciding which model i move forward with. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fd5ad6-6d59-4c1c-b9dc-57fc45606dcc",
   "metadata": {},
   "source": [
    "## Catboost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3fecf-f4bc-4474-8015-b4a3161b5528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCatBoost Regressor Performance:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 1,922.83\n",
      "Mean Squared Error  (MSE): 10,072,576\n",
      "R2 Score             (R2): 0.9422\n",
      "14.7 seconds to execute.\n",
      "\n",
      "\u001b[1mFeature Importance:\u001b[0m\n",
      "      Feature  Importance\n",
      "1       Model   28.995478\n",
      "0        Year   18.955490\n",
      "4        Trim   15.433566\n",
      "3     Mileage   14.942127\n",
      "5        Make   13.957470\n",
      "6  Body Style    6.524228\n",
      "2       State    0.753334\n",
      "7        City    0.438307\n"
     ]
    }
   ],
   "source": [
    "# Track training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train Catboost regressor model\n",
    "catboost_model = CatBoostRegressor(cat_features=categorical_features, verbose=0)\n",
    "catboost_model.fit(train_X,train_y)\n",
    "pred_catboost = catboost_model.predict(test_X)\n",
    "\n",
    "# Print performance\n",
    "print(\"\\033[1mCatBoost Regressor Performance:\\033[0m\")\n",
    "print(f'Mean Absolute Error (MAE): $ {metrics.mean_absolute_error(test_y, pred_catboost):,.2f}')\n",
    "print(f'Mean Squared Error  (MSE): {int(metrics.mean_squared_error(test_y, pred_catboost)):,}')\n",
    "print(f'R2 Score             (R2): {metrics.r2_score(test_y, pred_catboost):.4f}')\n",
    "\n",
    "# Print model execution time\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time)\n",
    "print(f\"{elapsed_time:.1f} seconds to execute.\")\n",
    "\n",
    "# Print feature importance\n",
    "print(\"\\n\\033[1mFeature Importance:\\033[0m\")\n",
    "importance_catboost = pd.DataFrame({\n",
    "    'Feature': train_X.columns,\n",
    "    'Importance': catboost_model.get_feature_importance()\n",
    "})\n",
    "importance_catboost = importance_catboost.sort_values(by='Importance', ascending=False)\n",
    "print(importance_catboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae53785-60de-431d-9d2b-bbe4eef94712",
   "metadata": {},
   "source": [
    "## LightGBM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dec1013-8d36-4704-a2b8-87fefda5da20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLightGBM Regressor Performance:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 1,695.51\n",
      "Mean Squared Error  (MSE): 8,147,484\n",
      "R2 Score             (R2): 0.9532\n",
      "0.4 seconds to execute.\n",
      "\n",
      "\u001b[1mFeature Importance:\u001b[0m\n",
      "      Feature  Importance\n",
      "1       Model         685\n",
      "4        Trim         607\n",
      "3     Mileage         569\n",
      "7        City         533\n",
      "0        Year         384\n",
      "2       State         135\n",
      "6  Body Style          44\n",
      "5        Make          43\n"
     ]
    }
   ],
   "source": [
    "# Track training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train LightGBM regressor model\n",
    "light_model = lgb.LGBMRegressor(n_jobs=-1, verbose=-1)\n",
    "light_model.fit(train_X, train_y, categorical_feature=categorical_features)\n",
    "pred_light = light_model.predict(test_X)\n",
    "\n",
    "# Print performance\n",
    "print(\"\\033[1mLightGBM Regressor Performance:\\033[0m\")\n",
    "print(f'Mean Absolute Error (MAE): $ {metrics.mean_absolute_error(test_y, pred_light):,.2f}')\n",
    "print(f'Mean Squared Error  (MSE): {int(metrics.mean_squared_error(test_y, pred_light)):,}')\n",
    "print(f'R2 Score             (R2): {metrics.r2_score(test_y, pred_light):.4f}')\n",
    "\n",
    "# Print model execution time\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time)\n",
    "print(f\"{elapsed_time:.1f} seconds to execute.\")\n",
    "\n",
    "# Print feature importance\n",
    "print(\"\\n\\033[1mFeature Importance:\\033[0m\")\n",
    "importance_light = pd.DataFrame({\n",
    "    'Feature': train_X.columns,\n",
    "    'Importance': light_model.feature_importances_\n",
    "})\n",
    "importance_light = importance_light.sort_values(by='Importance', ascending=False)\n",
    "print(importance_light)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5d3432-b055-4499-8fcf-33e8aacc5ea5",
   "metadata": {},
   "source": [
    "### Apply CatBoost to Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14746fc0-d187-4ff8-8c78-44ec5a0f7a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mCatBoost Regressor Performance on Validation Data from 8/15:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 2,371.04\n",
      "Mean Squared Error  (MSE): 14,785,259\n",
      "R2 Score             (R2): 0.9134\n",
      "\u001b[1m\n",
      "CatBoost Regressor Performance on Original Data from 7/21:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 1,922.83\n",
      "Mean Squared Error  (MSE): 10,072,576\n",
      "R2 Score             (R2): 0.9422\n"
     ]
    }
   ],
   "source": [
    "# Predict validation data\n",
    "pred_catboost_val = catboost_model.predict(X_validation)\n",
    "\n",
    "# Print validation performance\n",
    "print('\\n\\033[1mCatBoost Regressor Performance on Validation Data from 8/15:\\033[0m')\n",
    "print(f'Mean Absolute Error (MAE): $ {round(metrics.mean_absolute_error(y_validation, pred_catboost_val), 2):,}')\n",
    "print(f'Mean Squared Error  (MSE): {int(round(metrics.mean_squared_error(y_validation, pred_catboost_val))):,}')\n",
    "print(f'R2 Score             (R2): {round(metrics.r2_score(y_validation, pred_catboost_val), 4)}')\n",
    "\n",
    "# Print original performance\n",
    "print(\"\\033[1m\\nCatBoost Regressor Performance on Original Data from 7/21:\\033[0m\")\n",
    "print(f'Mean Absolute Error (MAE): $ {metrics.mean_absolute_error(test_y, pred_catboost):,.2f}')\n",
    "print(f'Mean Squared Error  (MSE): {int(metrics.mean_squared_error(test_y, pred_catboost)):,}')\n",
    "print(f'R2 Score             (R2): {metrics.r2_score(test_y, pred_catboost):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc1035a-a2ad-4e46-9040-06d37d309db8",
   "metadata": {},
   "source": [
    "### Apply LightGBM to Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f62b31b-504a-429c-bb44-77d8958dea85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mLightGBM Regressor Performance on Validation Data from 8/15:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 1,898.75\n",
      "Mean Squared Error  (MSE): 10,302,178\n",
      "R2 Score             (R2): 0.9397\n",
      "\u001b[1m\n",
      "LightGBM Regressor Performance on Original Data from 7/21:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 1,695.51\n",
      "Mean Squared Error  (MSE): 8,147,484\n",
      "R2 Score             (R2): 0.9532\n"
     ]
    }
   ],
   "source": [
    "# Predict validation data using the loaded model\n",
    "pred_light_val = light_model.predict(X_validation)\n",
    "\n",
    "# Print validation performance\n",
    "print('\\n\\033[1mLightGBM Regressor Performance on Validation Data from 8/15:\\033[0m')\n",
    "print(f'Mean Absolute Error (MAE): $ {round(metrics.mean_absolute_error(y_validation, pred_light_val), 2):,}')\n",
    "print(f'Mean Squared Error  (MSE): {int(round(metrics.mean_squared_error(y_validation, pred_light_val))):,}')\n",
    "print(f'R2 Score             (R2): {round(metrics.r2_score(y_validation, pred_light_val), 4)}')\n",
    "\n",
    "# Print original performance\n",
    "print(\"\\033[1m\\nLightGBM Regressor Performance on Original Data from 7/21:\\033[0m\")\n",
    "print(f'Mean Absolute Error (MAE): $ {metrics.mean_absolute_error(test_y, pred_light):,.2f}')\n",
    "print(f'Mean Squared Error  (MSE): {int(metrics.mean_squared_error(test_y, pred_light)):,}')\n",
    "print(f'R2 Score             (R2): {metrics.r2_score(test_y, pred_light):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34eec3e-df4d-46c1-88e7-9a2d4af6df36",
   "metadata": {},
   "source": [
    "## These data sets are signicantly simiplified with only 8 features and non encoding. All models perform very efficient. LightGBM has the best inital performance. I will move forward with these models and measure their validation on new data to find the best performing mdoel. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
