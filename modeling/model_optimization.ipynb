{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88391ebd-e794-4872-8863-239e242f5660",
   "metadata": {},
   "source": [
    "# Model Optimization\n",
    "\n",
    "### Objective:\n",
    "The objective of this notebook is to optimize the LightGBM model by tuning hyperparameters and improving its performance on car price predictions. The optimization process aims to reduce error metrics like Mean Absolute Error (MAE) and Mean Squared Error (MSE).\n",
    "\n",
    "### Summary:\n",
    "In this notebook, the LightGBM model was subjected to several optimization iterations using RandomizedSearchCV. The model initially performed well, but optimization was explored to determine if any further improvements could be made. After hyperparameter tuning, the final model showed minimal improvements in predictive performance. \n",
    "\n",
    "#### **Key Findings**:\n",
    "- **Legacy Model (XGBoost):**\n",
    "  - Mean Absolute Error (MAE): `$1,695.51`\n",
    "  - Mean Squared Error (MSE): `8,147,484`\n",
    "  - R² Score: `0.9532`\n",
    "  - Required manual encoding of categorical variables, which increased preprocessing time and complexity.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Optimized LightGBM Model**:\n",
    "  - **Final Performance After Optimization**:\n",
    "    - Mean Absolute Error (MAE): `$1,609.03`\n",
    "    - Mean Squared Error (MSE): `7,441,905`\n",
    "    - R² Score: `0.9573`\n",
    "\n",
    "<br>\n",
    "\n",
    " - **Improvement**:\n",
    "    - Optimized hyperparameters led to a reduced MAE by `$86.48` and MSE by `705,579` compared to XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df24ff1-52aa-411c-bbeb-58988e60e473",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Necessary libraries\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc564ee-41de-40a4-b607-bd2c4bfd0d1b",
   "metadata": {},
   "source": [
    "### Process Data and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04320a15-d4e9-4dcc-a6cd-d886d4f23501",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded & processed\n",
      "\n",
      "Train/Test Data Row & Column Count: (66316, 7) (16579, 7)\n",
      "\n",
      "LightGBM model trained\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df_cars = pd.read_csv('cleaned_data_july_21st.csv')\n",
    "\n",
    "# Define features and target\n",
    "features = ['Year', 'Model', 'State', 'Mileage', 'Trim', 'Make', 'Body Style']\n",
    "X = df_cars[features].copy()\n",
    "y = df_cars['Price']\n",
    "\n",
    "# Define and format categorical features\n",
    "categorical_features = ['Model', 'State', 'Trim', 'Make', 'Body Style']\n",
    "X[categorical_features] = X[categorical_features].astype('category')\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = joblib.load('final_scaler.pkl')\n",
    "X[['Year', 'Mileage']] = scaler.fit_transform(X[['Year', 'Mileage']])\n",
    "\n",
    "# Split data 80/20\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "print('Data loaded & processed')\n",
    "print('\\nTrain/Test Data Row & Column Count:', train_X.shape, test_X.shape)\n",
    "\n",
    "# Train LightGBM model for RandomizedSearch\n",
    "lgb_model = lgb.LGBMRegressor(n_jobs=-1, verbose=-1)\n",
    "lgb_model.fit(train_X, train_y, categorical_feature=categorical_features)\n",
    "print('\\nLightGBM model trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2fccba-c901-47c1-837a-696b025dddef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## LightGBM Hyperparameter Tuning w/ RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29ac967-36de-4300-84a3-5509e48d5244",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'subsample': 1.0, 'reg_lambda': 0.5, 'reg_alpha': 0.001, 'num_leaves': 100, 'n_estimators': 1000, 'max_depth': 5, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n",
      "\n",
      "\n",
      "\u001b[1mOptimized LightGBM Regression Performance:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 1,663.60\n",
      "Mean Squared Error  (MSE): 7,327,172\n",
      "R² Score             (R²): 0.9579\n",
      "37.4 minutes to execute.\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter search \n",
    "param_search = {\n",
    "    'num_leaves': [31, 50, 75, 100],\n",
    "    'max_depth': [5, 7, 9, 12, 15],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [250, 500, 1000],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "    'reg_alpha': [0, 0.001, 0.1, 0.5],\n",
    "    'reg_lambda': [0, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "# Track training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform RandomizedSearch with cross-validation\n",
    "random_search_light = RandomizedSearchCV(\n",
    "    lgb_model, \n",
    "    param_distributions=param_search, \n",
    "    n_iter=300, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "# Fit model with best hyperparameters\n",
    "light_model_optimized = random_search_light.fit(train_X, train_y)\n",
    "\n",
    "# Predict test data\n",
    "pred_light_optimized = light_model_optimized.predict(test_X)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best Hyperparameters:\", light_model_optimized.best_params_)\n",
    "print('\\n')\n",
    "\n",
    "# Print best hyperparameters LightGBM model performance\n",
    "print(\"\\033[1mOptimized LightGBM Model Performance:\\033[0m\")\n",
    "print(f'Mean Absolute Error (MAE): $ {metrics.mean_absolute_error(test_y, pred_light_optimized):,.2f}')\n",
    "print(f'Mean Squared Error  (MSE): {int(metrics.mean_squared_error(test_y, pred_light_optimized)):,}')\n",
    "print(f'R² Score             (R²): {metrics.r2_score(test_y, pred_light_optimized):.4f}')\n",
    "\n",
    "# Print search execution time\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time) / 60\n",
    "print(f\"{elapsed_time:.1f} minutes to execute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9e7f77-4806-4bcb-8157-ce296961cc85",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c7ae85-5ac5-4751-94fb-084604a15481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mFinal Model (LightGBM) Performance:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 1,663.60\n",
      "Mean Squared Error  (MSE): 7,327,172\n",
      "R² Score             (R²): 0.9579\n",
      "1.1 seconds to execute.\n",
      "\n",
      "\u001b[1mPreliminary LightGBM Regressor Performance:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 1,695.51\n",
      "Mean Squared Error  (MSE): 8,147,484\n",
      "R² Score             (R²): 0.9532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['final_model.pkl']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define final/best hyperparameters\n",
    "best_hyperparams = {\n",
    "    'subsample': 1.0, \n",
    "    'reg_lambda': 0.5, \n",
    "    'reg_alpha': 0.001, \n",
    "    'num_leaves': 100, \n",
    "    'n_estimators': 1000, \n",
    "    'max_depth': 5, \n",
    "    'learning_rate': 0.1, \n",
    "    'colsample_bytree': 0.9\n",
    "}\n",
    "\n",
    "# Track training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train final optimized LightGBM model\n",
    "final_model = lgb.LGBMRegressor(**best_hyperparams, verbose=-1, n_jobs=-1)\n",
    "final_model.fit(train_X, train_y, categorical_feature=categorical_features)\n",
    "\n",
    "# Predict test data\n",
    "final_pred = final_model.predict(test_X)\n",
    "\n",
    "# Print final optimized LightGBM model performance\n",
    "print('\\033[1mOptimized LightGBM Model Performance (Final Model):\\033[0m')\n",
    "print(f'Mean Absolute Error (MAE): $ {metrics.mean_absolute_error(test_y, final_pred):,.2f}')\n",
    "print(f'Mean Squared Error  (MSE): {int(metrics.mean_squared_error(test_y, final_pred)):,}')\n",
    "print(f'R² Score             (R²): {metrics.r2_score(test_y, final_pred):.4f}')\n",
    "\n",
    "# Print final model execution time\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time)\n",
    "print(f\"{elapsed_time:.1f} seconds to execute.\")\n",
    "\n",
    "# Print initial LightGBM performance\n",
    "print('\\n\\033[1mPreliminary LightGBM Model Performance:\\033[0m')\n",
    "print('Mean Absolute Error (MAE): $ 1,695.51')\n",
    "print('Mean Squared Error  (MSE): 8,147,484')\n",
    "print('R² Score             (R²): 0.9532')\n",
    "\n",
    "# Save final optimized LightGBM model\n",
    "joblib.dump(final_model, 'final_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fece6cc2-481a-464f-85e6-085b7aacf485",
   "metadata": {},
   "source": [
    "## **Conclusion**:\n",
    "The optimization of the LightGBM model successfully reduced both MAE and MSE while improving the model's overall performance. Its ability to handle categorical features without encoding resulted in faster training times and more efficient preprocessing. Compared to the legacy XGBoost model, LightGBM's optimized version proved to be more accurate and efficient, making it a superior choice for predicting vehicle prices. The optimized LightGBM model is now better suited for deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
