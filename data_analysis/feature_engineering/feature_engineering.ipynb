{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ce9809-7675-43e3-a4e7-7e174f136518",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f60d2d-7e29-45f3-9875-5447044afdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import itertools\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9395cf25-c64e-4cc2-8b79-a5d76a290724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded & processed\n"
     ]
    }
   ],
   "source": [
    "# Load and process data\n",
    "df_cars = pd.read_csv('cleaned_data_july_21st.csv')\n",
    "\n",
    "# Define possible features to be used in model combinations\n",
    "possible_features = ['Year', 'Model', 'State', 'Mileage', 'Trim', 'Make', 'Body Style', 'City']\n",
    "\n",
    "print('Data loaded & processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3914280c-df9b-4b87-a1a6-795e9a1f7c5d",
   "metadata": {},
   "source": [
    "## Test All Feature Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f0b03-1987-496d-b3f7-c70d7fc67a47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mBest LightGBM Performance for 2 Features:\u001b[0m\n",
      "Features: ['Year', 'Model']\n",
      "Mean Absolute Error (MAE): $ 3195.93\n",
      "Mean Squared Error  (MSE): 23,809,094\n",
      "R2 Score             (R2): 0.8633\n",
      "Execution Time: 0.2 seconds\n",
      "\n",
      "\u001b[1mBest LightGBM Performance for 3 Features:\u001b[0m\n",
      "Features: ['Year', 'Model', 'Trim']\n",
      "Mean Absolute Error (MAE): $ 2382.08\n",
      "Mean Squared Error  (MSE): 12,685,900\n",
      "R2 Score             (R2): 0.9271\n",
      "Execution Time: 0.2 seconds\n",
      "\n",
      "\u001b[1mBest LightGBM Performance for 4 Features:\u001b[0m\n",
      "Features: ['Year', 'Model', 'Mileage', 'Trim']\n",
      "Mean Absolute Error (MAE): $ 1789.99\n",
      "Mean Squared Error  (MSE): 8,485,671\n",
      "R2 Score             (R2): 0.9513\n",
      "Execution Time: 0.2 seconds\n",
      "\n",
      "\u001b[1mBest LightGBM Performance for 5 Features:\u001b[0m\n",
      "Features: ['Year', 'Model', 'Mileage', 'Trim', 'Body Style']\n",
      "Mean Absolute Error (MAE): $ 1796.36\n",
      "Mean Squared Error  (MSE): 8,257,580\n",
      "R2 Score             (R2): 0.9526\n",
      "Execution Time: 0.2 seconds\n",
      "\n",
      "\u001b[1mBest LightGBM Performance for 6 Features:\u001b[0m\n",
      "Features: ['Year', 'Model', 'State', 'Mileage', 'Trim', 'Body Style']\n",
      "Mean Absolute Error (MAE): $ 1732.61\n",
      "Mean Squared Error  (MSE): 8,153,008\n",
      "R2 Score             (R2): 0.9532\n",
      "Execution Time: 0.2 seconds\n",
      "\n",
      "\u001b[1mBest LightGBM Performance for 7 Features:\u001b[0m\n",
      "Features: ['Year', 'Model', 'State', 'Mileage', 'Trim', 'Make', 'Body Style']\n",
      "Mean Absolute Error (MAE): $ 1735.20\n",
      "Mean Squared Error  (MSE): 8,088,164\n",
      "R2 Score             (R2): 0.9536\n",
      "Execution Time: 0.2 seconds\n",
      "\n",
      "\u001b[1mBest LightGBM Performance for 8 Features:\u001b[0m\n",
      "Features: ['Year', 'Model', 'State', 'Mileage', 'Trim', 'Make', 'Body Style', 'City']\n",
      "Mean Absolute Error (MAE): $ 1695.51\n",
      "Mean Squared Error  (MSE): 8,147,484\n",
      "R2 Score             (R2): 0.9532\n",
      "Execution Time: 0.4 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_feature_combinations(df_cars, target, possible_features):\n",
    "    \"\"\"\n",
    "    Train and evaluate a LightGBM model using different combinations of features.\n",
    "    \"\"\"\n",
    "    best_results = {}\n",
    "\n",
    "    # Iterate over all possible feature combos\n",
    "    for r in range(2, len(possible_features) + 1):\n",
    "        best_result = None\n",
    "\n",
    "        for combo in itertools.combinations(possible_features, r):\n",
    "            combo_list = list(combo)\n",
    "\n",
    "            # Specify your features and target\n",
    "            X = df_cars[combo_list].copy()\n",
    "            y = df_cars[target]\n",
    "\n",
    "            # Specify categorical features\n",
    "            categorical_features = [col for col in combo_list if col in ['Model', 'State', 'Trim', 'Make', 'Body Style', 'City']]\n",
    "            \n",
    "            # Converting categorical features to 'category' dtype\n",
    "            X[categorical_features] = X[categorical_features].astype('category')\n",
    "\n",
    "            # Scale numerical features if they are in the combo\n",
    "            numerical_cols = [col for col in ['Year', 'Mileage'] if col in combo_list]\n",
    "            if numerical_cols:\n",
    "                scaler = StandardScaler()\n",
    "                X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "\n",
    "            # Split the data\n",
    "            train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "            # Track training time\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Train LightGBM model on current feature combo\n",
    "            lightgbm_model = lgb.LGBMRegressor(n_jobs=-1, verbose=-1)\n",
    "            lightgbm_model.fit(train_X, train_y, categorical_feature=categorical_features)\n",
    "\n",
    "            # Predict on test data\n",
    "            pred_lightgbm = lightgbm_model.predict(test_X)\n",
    "            mae = metrics.mean_absolute_error(test_y, pred_lightgbm)\n",
    "            mse = metrics.mean_squared_error(test_y, pred_lightgbm)\n",
    "            r2 = metrics.r2_score(test_y, pred_lightgbm)\n",
    "\n",
    "            end_time = time.time()\n",
    "            elapsed_time = (end_time - start_time)\n",
    "\n",
    "            # Check if this is the best result for the current feature count\n",
    "            if best_result is None or r2 > best_result['R2']:\n",
    "                best_result = {\n",
    "                    'features': combo_list,\n",
    "                    'MAE': mae,\n",
    "                    'MSE': mse,\n",
    "                    'R2': r2,\n",
    "                    'time': elapsed_time\n",
    "                }\n",
    "\n",
    "        # Store the best result for this feature count\n",
    "        if best_result:\n",
    "            best_results[r] = best_result\n",
    "\n",
    "    return best_results\n",
    "\n",
    "# Run feature combination evaluation\n",
    "best_results = evaluate_feature_combinations(df_cars, 'Price', possible_features)\n",
    "\n",
    "# Print the best feature combinations for each feature count\n",
    "for feature_count, result in best_results.items():\n",
    "    print(f\"\\033[1mBest LightGBM Performance for {feature_count} Features:\\033[0m\")\n",
    "    print(f\"Features: {result['features']}\")\n",
    "    print(f'Mean Absolute Error (MAE): $ {result[\"MAE\"]:.2f}')\n",
    "    print(f'Mean Squared Error  (MSE): {int(result[\"MSE\"]):,}')\n",
    "    print(f'R2 Score             (R2): {result[\"R2\"]:.4f}')\n",
    "    print(f\"Execution Time: {result['time']:.1f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e82a94-9a42-407f-8bff-56ec505d4bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "biggest jump in terms of mae and mse comes at 4 features. the returns begin to diminsh after that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c939c2-7a9a-4353-aaf8-6bf765f36a96",
   "metadata": {},
   "source": [
    "## **Conclusion**: \n",
    "\n",
    "8 features has the best mae but 7 features has the best mse. Removing city removes nearly 3,000 unique values, and makes my model more efficient. i will apply 7 features to the validation set to test test the resutls to see which model i should optimize. apply to validaiton set will help tell me which model is generalizing the data better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0787882d-5d5a-49ec-b1b1-1eb58f055414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m(7 Features) LightGBM Regressor Performance on Validation Data from 8/15:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 1,913.13\n",
      "Mean Squared Error  (MSE): 9,884,403\n",
      "R2 Score             (R2): 0.9421\n",
      "\n",
      "\u001b[1m(8 Features) LightGBM Regressor Performance on Validation Data from 8/15:\n",
      "Mean Absolute Error (MAE): $ 1,898.75\n",
      "Mean Squared Error  (MSE): 10,302,178\n",
      "R2 Score             (R2): 0.9397\n"
     ]
    }
   ],
   "source": [
    "# Define features and target\n",
    "features = ['Year', 'Model', 'State', 'Mileage', 'Trim', 'Make', 'Body Style']\n",
    "X = df_cars[features].copy()\n",
    "y = df_cars['Price']\n",
    "\n",
    "# Specify categorical features\n",
    "categorical_features = ['Model', 'State', 'Trim', 'Make', 'Body Style']\n",
    "\n",
    "# Converting categorical features (XGB & LightGBM use category, Catboost uses string)\n",
    "X[categorical_features] = X[categorical_features].astype('category')\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X[['Year', 'Mileage']] = scaler.fit_transform(X[['Year', 'Mileage']])\n",
    "\n",
    "# Split the data\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "lightgbm_model = lgb.LGBMRegressor(n_jobs=-1, verbose=-1)\n",
    "lightgbm_model.fit(train_X, train_y, categorical_feature=categorical_features)\n",
    "\n",
    "# Load validation data\n",
    "df_validation = pd.read_csv('cleaned_data_aug_16th.csv') \n",
    "\n",
    "# Drop irrelevant columns\n",
    "df_validation.drop(columns=['Listing ID', 'Stock Type'], inplace=True)\n",
    "\n",
    "# Ensure non-numerical columns are formatted as 'category'\n",
    "df_validation[categorical_features] = df_validation[categorical_features].astype('category')\n",
    "\n",
    "# Scale the numerical features with saved scaler\n",
    "X_validation = df_validation[features].copy()\n",
    "X_validation[['Year', 'Mileage']] = scaler.transform(X_validation[['Year', 'Mileage']])\n",
    "X_validation[['Year', 'Mileage']] = X_validation[['Year', 'Mileage']].astype('float64')\n",
    "\n",
    "# Predict validation data using the loaded model\n",
    "pred_light = lightgbm_model.predict(X_validation)\n",
    "\n",
    "# Define validation target variable\n",
    "y_validation = df_validation['Price'].values\n",
    "\n",
    "# Print validation data performance\n",
    "print('\\n\\033[1m(7 Features) LightGBM Regressor Performance on Validation Data from 8/15:\\033[0m')\n",
    "print(f'Mean Absolute Error (MAE): $ {round(metrics.mean_absolute_error(y_validation, pred_light), 2):,}')\n",
    "print(f'Mean Squared Error  (MSE): {int(round(metrics.mean_squared_error(y_validation, pred_light))):,}')\n",
    "print(f'R2 Score             (R2): {round(metrics.r2_score(y_validation, pred_light), 4)}')\n",
    "\n",
    "print('\\n\\033[1m(8 Features) LightGBM Regressor Performance on Validation Data from 8/15:')\n",
    "print('Mean Absolute Error (MAE): $ 1,898.75')\n",
    "print('Mean Squared Error  (MSE): 10,302,178')\n",
    "print('R2 Score             (R2): 0.9397')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f226ec69-4172-47f7-a295-fea898fc38e3",
   "metadata": {},
   "source": [
    "## **Conclusion**:\n",
    "\n",
    "7 Features still appears to be the best combination. The difference in mae in neglible in comparison to target range and the better mse shows its handling large errors well and likely better at generalizing when introduced to new data. 7 Features also reduces unique values for categroical columns by more than 3,000. 7 features captures the best range of everything. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
