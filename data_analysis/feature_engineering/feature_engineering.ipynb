{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ce9809-7675-43e3-a4e7-7e174f136518",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "### Objective\n",
    "\n",
    "The goal of this notebook is to conduct feature engineering by testing the predictive performance of different combinations of features to optimize the model's accuracy and generalizability. I aim to identify the best-performing feature set that balances Mean Absolute Error (MAE) and Mean Squared Error (MSE).\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "After evaluating various feature combinations, the results indicated that both 7 and 8 features achieved the best performance on the test data. However, upon applying these models to the validation set, the model with 7 features demonstrated better generalization capabilities. Although the 8-feature model had a slightly better MAE, the 7-feature model exhibited a lower MSE, suggesting it handles large errors more effectively. Given that MAE was relatively close for both models, the 7-feature model was chosen for its superior ability to generalize to new data, making it the best candidate moving forward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f60d2d-7e29-45f3-9875-5447044afdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import itertools\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9395cf25-c64e-4cc2-8b79-a5d76a290724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded & processed\n"
     ]
    }
   ],
   "source": [
    "# Load and process data\n",
    "df_cars = pd.read_csv('cleaned_data_july_21st.csv')\n",
    "\n",
    "# Define possible features to be used in model combinations\n",
    "possible_features = ['Year', 'Model', 'State', 'Mileage', 'Trim', 'Make', 'Body Style', 'City']\n",
    "\n",
    "print('Data loaded & processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3914280c-df9b-4b87-a1a6-795e9a1f7c5d",
   "metadata": {},
   "source": [
    "## Test All Feature Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f0b03-1987-496d-b3f7-c70d7fc67a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mBest LightGBM Performance for 2 Features:\u001b[0m\n",
      "Features: ['Year', 'Model']\n",
      "Mean Absolute Error (MAE): $ 3195.93\n",
      "Mean Squared Error  (MSE): 23,809,094\n",
      "R2 Score             (R2): 0.8633\n",
      "Execution Time: 0.2 seconds\n",
      "\n",
      "\u001b[1mBest LightGBM Performance for 3 Features:\u001b[0m\n",
      "Features: ['Year', 'Model', 'Trim']\n",
      "Mean Absolute Error (MAE): $ 2382.08\n",
      "Mean Squared Error  (MSE): 12,685,900\n",
      "R2 Score             (R2): 0.9271\n",
      "Execution Time: 0.2 seconds\n",
      "\n",
      "\u001b[1mBest LightGBM Performance for 4 Features:\u001b[0m\n",
      "Features: ['Year', 'Model', 'Mileage', 'Trim']\n",
      "Mean Absolute Error (MAE): $ 1789.99\n",
      "Mean Squared Error  (MSE): 8,485,671\n",
      "R2 Score             (R2): 0.9513\n",
      "Execution Time: 0.2 seconds\n",
      "\n",
      "\u001b[1mBest LightGBM Performance for 5 Features:\u001b[0m\n",
      "Features: ['Year', 'Model', 'Mileage', 'Trim', 'Body Style']\n",
      "Mean Absolute Error (MAE): $ 1796.36\n",
      "Mean Squared Error  (MSE): 8,257,580\n",
      "R2 Score             (R2): 0.9526\n",
      "Execution Time: 0.2 seconds\n",
      "\n",
      "\u001b[1mBest LightGBM Performance for 6 Features:\u001b[0m\n",
      "Features: ['Year', 'Model', 'State', 'Mileage', 'Trim', 'Body Style']\n",
      "Mean Absolute Error (MAE): $ 1732.61\n",
      "Mean Squared Error  (MSE): 8,153,008\n",
      "R2 Score             (R2): 0.9532\n",
      "Execution Time: 0.2 seconds\n",
      "\n",
      "\u001b[1mBest LightGBM Performance for 7 Features:\u001b[0m\n",
      "Features: ['Year', 'Model', 'State', 'Mileage', 'Trim', 'Make', 'Body Style']\n",
      "Mean Absolute Error (MAE): $ 1735.20\n",
      "Mean Squared Error  (MSE): 8,088,164\n",
      "R2 Score             (R2): 0.9536\n",
      "Execution Time: 0.2 seconds\n",
      "\n",
      "\u001b[1mBest LightGBM Performance for 8 Features:\u001b[0m\n",
      "Features: ['Year', 'Model', 'State', 'Mileage', 'Trim', 'Make', 'Body Style', 'City']\n",
      "Mean Absolute Error (MAE): $ 1695.51\n",
      "Mean Squared Error  (MSE): 8,147,484\n",
      "R2 Score             (R2): 0.9532\n",
      "Execution Time: 0.4 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_feature_combinations(df_cars, target, possible_features):\n",
    "    \"\"\"\n",
    "    Train and evaluate a LightGBM model using different combinations of features.\n",
    "    \"\"\"\n",
    "    best_results = {}\n",
    "\n",
    "    # Iterate over all possible feature combos\n",
    "    for r in range(2, len(possible_features) + 1):\n",
    "        best_result = None\n",
    "\n",
    "        for combo in itertools.combinations(possible_features, r):\n",
    "            combo_list = list(combo)\n",
    "\n",
    "            # Specify your features and target\n",
    "            X = df_cars[combo_list].copy()\n",
    "            y = df_cars[target]\n",
    "\n",
    "            # Specify categorical features\n",
    "            categorical_features = [col for col in combo_list if col in ['Model', 'State', 'Trim', 'Make', 'Body Style', 'City']]\n",
    "            \n",
    "            # Converting categorical features to 'category' dtype\n",
    "            X[categorical_features] = X[categorical_features].astype('category')\n",
    "\n",
    "            # Scale numerical features if they are in the combo\n",
    "            numerical_cols = [col for col in ['Year', 'Mileage'] if col in combo_list]\n",
    "            if numerical_cols:\n",
    "                scaler = StandardScaler()\n",
    "                X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "\n",
    "            # Split the data\n",
    "            train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "            # Track training time\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Train LightGBM model on current feature combo\n",
    "            lightgbm_model = lgb.LGBMRegressor(n_jobs=-1, verbose=-1)\n",
    "            lightgbm_model.fit(train_X, train_y, categorical_feature=categorical_features)\n",
    "\n",
    "            # Predict on test data\n",
    "            pred_lightgbm = lightgbm_model.predict(test_X)\n",
    "            mae = metrics.mean_absolute_error(test_y, pred_lightgbm)\n",
    "            mse = metrics.mean_squared_error(test_y, pred_lightgbm)\n",
    "            r2 = metrics.r2_score(test_y, pred_lightgbm)\n",
    "\n",
    "            end_time = time.time()\n",
    "            elapsed_time = (end_time - start_time)\n",
    "\n",
    "            # Check if this is the best result for the current feature count\n",
    "            if best_result is None or r2 > best_result['R2']:\n",
    "                best_result = {\n",
    "                    'features': combo_list,\n",
    "                    'MAE': mae,\n",
    "                    'MSE': mse,\n",
    "                    'R2': r2,\n",
    "                    'time': elapsed_time\n",
    "                }\n",
    "\n",
    "        # Store the best result for this feature count\n",
    "        if best_result:\n",
    "            best_results[r] = best_result\n",
    "\n",
    "    return best_results\n",
    "\n",
    "# Run feature combination evaluation\n",
    "best_results = evaluate_feature_combinations(df_cars, 'Price', possible_features)\n",
    "\n",
    "# Print the best feature combinations for each feature count\n",
    "for feature_count, result in best_results.items():\n",
    "    print(f\"\\033[1mBest LightGBM Performance for {feature_count} Features:\\033[0m\")\n",
    "    print(f\"Features: {result['features']}\")\n",
    "    print(f'Mean Absolute Error (MAE): $ {result[\"MAE\"]:.2f}')\n",
    "    print(f'Mean Squared Error  (MSE): {int(result[\"MSE\"]):,}')\n",
    "    print(f'R2 Score             (R2): {result[\"R2\"]:.4f}')\n",
    "    print(f\"Execution Time: {result['time']:.1f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c939c2-7a9a-4353-aaf8-6bf765f36a96",
   "metadata": {},
   "source": [
    "## Analysis Checkpoint\n",
    "\n",
    "After testing various combinations of features (from 2 to 8), I observed that the 8-feature model produced the best MAE, while the 7-feature model achieved the best MSE. The biggest performance improvement occurred when increasing from 3 to 4 features, after which the improvements started to diminish\n",
    "\n",
    "The removal of the `City` feature, which has nearly 3,000 unique values, improves model efficiency without compromising much on accuracy. Moving forward, I plan to compare the 7- and 8-feature models by applying them to the validation data. This comparison will help determine which model generalizes better and is more suitable for further optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def27282-cd0c-48e9-90a1-8e1acb50e167",
   "metadata": {},
   "source": [
    "# Validation Data Performance (7 vs 8 Feature Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0787882d-5d5a-49ec-b1b1-1eb58f055414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m(7 Features) LightGBM Regressor Performance on Validation Data from 8/15:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 1,913.13\n",
      "Mean Squared Error  (MSE): 9,884,403\n",
      "R2 Score             (R2): 0.9421\n",
      "\n",
      "\u001b[1m(8 Features) LightGBM Regressor Performance on Validation Data from 8/15:\u001b[0m\n",
      "Mean Absolute Error (MAE): $ 1,898.75\n",
      "Mean Squared Error  (MSE): 10,302,178\n",
      "R2 Score             (R2): 0.9397\n"
     ]
    }
   ],
   "source": [
    "# Define features and target\n",
    "features = ['Year', 'Model', 'State', 'Mileage', 'Trim', 'Make', 'Body Style']\n",
    "features8 = ['Year', 'Model', 'State', 'Mileage', 'Trim', 'Make', 'Body Style', 'City']\n",
    "X = df_cars[features].copy()\n",
    "X8 = df_cars[features8].copy()\n",
    "y = df_cars['Price']\n",
    "\n",
    "# Define categorical features\n",
    "categorical_features = ['Model', 'State', 'Trim', 'Make', 'Body Style']\n",
    "categorical_features8 = ['Model', 'State', 'Trim', 'Make', 'Body Style', 'City']\n",
    "\n",
    "# Format categorical features\n",
    "X[categorical_features] = X[categorical_features].astype('category')\n",
    "X8[categorical_features8] = X8[categorical_features8].astype('category')\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X[['Year', 'Mileage']] = scaler.fit_transform(X[['Year', 'Mileage']])\n",
    "X8[['Year', 'Mileage']] = scaler.fit_transform(X8[['Year', 'Mileage']])\n",
    "\n",
    "# Split data\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "train_X8, test_X8, train_y, test_y = train_test_split(X8, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# 7 feature lightgbm model\n",
    "lightgbm_model = lgb.LGBMRegressor(n_jobs=-1, verbose=-1)\n",
    "lightgbm_model.fit(train_X, train_y, categorical_feature=categorical_features)\n",
    "\n",
    "# 8 feature lightgbm model\n",
    "lightgbm_model8 = lgb.LGBMRegressor(n_jobs=-1, verbose=-1)\n",
    "lightgbm_model8.fit(train_X8, train_y, categorical_feature=categorical_features8)\n",
    "\n",
    "# Load validation data\n",
    "df_validation = pd.read_csv('cleaned_data_aug_16th.csv') \n",
    "df_validation8 = pd.read_csv('cleaned_data_aug_16th.csv') \n",
    "\n",
    "# Format categorical columns\n",
    "df_validation[categorical_features] = df_validation[categorical_features].astype('category')\n",
    "df_validation8[categorical_features8] = df_validation8[categorical_features8].astype('category')\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X[['Year', 'Mileage']] = scaler.fit_transform(X[['Year', 'Mileage']])\n",
    "X8[['Year', 'Mileage']] = scaler.transform(X8[['Year', 'Mileage']])  # Reuse the already fitted scaler\n",
    "\n",
    "# Predict validation data 7 & 8 feature models\n",
    "pred_light = lightgbm_model.predict(X_validation)\n",
    "pred_light8 = lightgbm_model8.predict(X_validation8)\n",
    "\n",
    "# Define target\n",
    "y_validation = df_validation['Price'].values\n",
    "\n",
    "# Print validation data performance with 7 & 8 feature models\n",
    "print('\\n\\033[1m(7 Features) LightGBM Regressor Performance on Validation Data from 8/15:\\033[0m')\n",
    "print(f'Mean Absolute Error (MAE): $ {round(metrics.mean_absolute_error(y_validation, pred_light), 2):,}')\n",
    "print(f'Mean Squared Error  (MSE): {int(round(metrics.mean_squared_error(y_validation, pred_light))):,}')\n",
    "print(f'R2 Score             (R2): {round(metrics.r2_score(y_validation, pred_light), 4)}')\n",
    "\n",
    "print('\\n\\033[1m(8 Features) LightGBM Regressor Performance on Validation Data from 8/15:\\033[0m')\n",
    "print(f'Mean Absolute Error (MAE): $ {round(metrics.mean_absolute_error(y_validation, pred_light8), 2):,}')\n",
    "print(f'Mean Squared Error  (MSE): {int(round(metrics.mean_squared_error(y_validation, pred_light8))):,}')\n",
    "print(f'R2 Score             (R2): {round(metrics.r2_score(y_validation, pred_light8), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f226ec69-4172-47f7-a295-fea898fc38e3",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "After testing various feature combinations, the 7-feature model was found to be the best in terms of generalization and performance. Although the 8-feature model had a slightly better MAE on the test set, the 7-feature model demonstrated a better MSE, indicating it is more effective at handling large errors. \n",
    "\n",
    "In conclusion, the 7-feature model strikes the optimal balance between accuracy and generalization, making it the ideal candidate for further optimization and eventual deployment. This model will be used in subsequent steps to fine-tune hyperparameters and maximize predictive performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
